# Training configuration for ByT5 fine-tuning

# Model
model:
  name: google/byt5-small  # Options: google/byt5-small, google/byt5-base, google/byt5-large
  max_input_length: 1024   # ByT5 max sequence length (bytes) - needs headroom for RAG context + diacritics
  max_output_length: 256   # Max translation length

# Data
data:
  train_corpus: data/processed/combined_corpus.csv
  validation_split: 0.10  # 10% for validation (more important with small corpus)
  test_split: 0.10  # 10% for testing

  # Dataset statistics (actual, 2026-02-02):
  # - Total: 1,589 pairs (after Phase 1 extraction)
  # - Train: ~1,270 pairs (80%)
  # - Validation: ~160 pairs (10%)
  # - Test: ~160 pairs (10%)

  # Augmentation
  augmentation:
    enabled: true
    synthetic_gaps:
      enabled: true
      probability: 0.3  # 30% of examples get synthetic gaps
      gap_length_range: [1, 5]  # Mask 1-5 consecutive words
    back_translation:
      enabled: false  # Enable if back-translation model available

# Retrieval (RAG)
retrieval:
  enabled: true
  corpus_path: data/processed/combined_corpus.csv
  index_path: data/indices/faiss_index.bin
  k_examples: 3  # Retrieve top-3 similar examples (smaller corpus)
  max_context_length: 800  # Max bytes for RAG context

# Training hyperparameters
training:
  # Optimizer
  learning_rate: 5.0e-5
  weight_decay: 0.01
  warmup_steps: 50       # ~6 % of 800 total optimizer steps (1270 train / eff-batch 16 * 10 epochs)

  # Batch size
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 2  # Effective batch size = 8 * 2 = 16

  # Epochs and steps
  num_train_epochs: 10
  max_steps: -1  # -1 means use num_train_epochs

  # Evaluation
  evaluation_strategy: steps
  eval_steps: 100        # Evaluate ~8x during a full 10-epoch run
  save_strategy: steps
  save_steps: 100        # Save ~8x, keep best 3 via save_total_limit
  save_total_limit: 3  # Keep only 3 best checkpoints

  # Early stopping
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  early_stopping_patience: 3  # Stop if no improvement for 3 eval steps

# Hardware
hardware:
  fp16: true  # Mixed precision training (faster, less memory)
  gradient_checkpointing: true
  dataloader_num_workers: 4

# Logging and output
output:
  output_dir: models/byt5_finetuned
  logging_dir: logs/training
  logging_steps: 100
  report_to: null  # Options: wandb, tensorboard, null

# Reproducibility
seed: 42

# Advanced: Multi-task learning (optional)
multi_task:
  enabled: false
  tasks:
    - translation
    - lemmatization  # Predict lemmatized forms
    - pos_tagging    # Predict part-of-speech tags
