{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 121150,
          "databundleVersionId": 15061024,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31239,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Akkadian Translation Challenge",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "k9hQkwNKnbQM"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "deep_past_initiative_machine_translation_path = kagglehub.competition_download('deep-past-initiative-machine-translation')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "L6UpZQ7znbQO"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-06T01:17:20.177912Z",
          "iopub.execute_input": "2026-01-06T01:17:20.178166Z",
          "iopub.status.idle": "2026-01-06T01:17:22.479461Z",
          "shell.execute_reply.started": "2026-01-06T01:17:20.178144Z",
          "shell.execute_reply": "2026-01-06T01:17:22.478395Z"
        },
        "id": "Bd6S2clYnbQO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This is what the sample should look like when we submit\n",
        "sample_df = pd.read_csv('/kaggle/input/deep-past-initiative-machine-translation/sample_submission.csv')\n",
        "sample_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-06T01:26:22.055379Z",
          "iopub.execute_input": "2026-01-06T01:26:22.055726Z",
          "iopub.status.idle": "2026-01-06T01:26:22.068639Z",
          "shell.execute_reply.started": "2026-01-06T01:26:22.055675Z",
          "shell.execute_reply": "2026-01-06T01:26:22.067654Z"
        },
        "id": "m_FGKgRHnbQO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# view what is inside each of the CSVs\n",
        "train_df = pd.read_csv('/kaggle/input/deep-past-initiative-machine-translation/train.csv')\n",
        "train_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-06T01:38:24.870718Z",
          "iopub.execute_input": "2026-01-06T01:38:24.871631Z",
          "iopub.status.idle": "2026-01-06T01:38:24.915742Z",
          "shell.execute_reply.started": "2026-01-06T01:38:24.871603Z",
          "shell.execute_reply": "2026-01-06T01:38:24.914752Z"
        },
        "id": "_3bFnosVnbQO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('/kaggle/input/deep-past-initiative-machine-translation/test.csv')\n",
        "test_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-06T01:38:47.175272Z",
          "iopub.execute_input": "2026-01-06T01:38:47.175883Z",
          "iopub.status.idle": "2026-01-06T01:38:47.18906Z",
          "shell.execute_reply.started": "2026-01-06T01:38:47.175852Z",
          "shell.execute_reply": "2026-01-06T01:38:47.188133Z"
        },
        "id": "Cz0anb3knbQO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Information on Provided Documents\n",
        "\n",
        "1. ```bibliography.csv``` contains the citations for each work presented in the dataset\n",
        "2. ```publications.csv``` contains additional publications for works, with a column ```has_akkadian``` to tell if the work contains Akkadian\n",
        "3. ```Sentences_Oare_FirstWord_LinNum.csv``` contains data on the first word of a sentence and the line number it occurs on per sample of text\n",
        "4. ```OA_Lexicon_eBL.csv``` contains type, form, norm, and lexeme of given words from the Akkadian language.\n",
        "5. ```eBL_Dictionary.csv``` contains words, their definitions, and their derived forms of given words\n",
        "6. ```train.csv``` contains oare_id, transliteration, and translation of given sentences of Akkadian sentences\n",
        "7. ```test.csv``` contains the id, text_id, line_start/line_end of the sentence, and the transliteration -> I need to solve for the translation.\n",
        "8.  ```published_texts.csv``` contains oare_id, cdli_id, oatp_key, eBL_id, original transliteration, and the transliteration of the texts\n",
        "9.  ```resources.csv``` contains the resource information for the texts provided\n",
        "10.  ```sample_submission.csv``` details the format that the output data must be presented in to be scored"
      ],
      "metadata": {
        "id": "_aORqKnmnbQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Data Preprocessing\n",
        "\n",
        "Following the suggested workflow will help us add more training data to the model, which we can then use for machine translation.\n",
        "\n",
        "1. **Locate each text and its translation:** Use the document identifiers (IDs, aliases, or museum numbers) to match transliterations with their corresponding translations in the OCR output.\n",
        "2. **Convert all translations to English:** The source translations may appear in multiple languages (e.g., English, French, German, Turkish). For consistency, convert everything to English.\n",
        "3. **Create sentence-level alignments:** Break both the Akkadian transliteration and the matching English translation into sentences and align them pairwise. This sentence-level mapping is the most useful format for training and evaluating machine translation models.\n",
        "\n",
        "Once these steps are followed, we now have a massive training set to then use within the model."
      ],
      "metadata": {
        "id": "hmaRNX-lnbQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Locate each text and its translation\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "PRvNNFpdnbQP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Convert all translations to English (the source\n",
        "# translations may appear in multiple languages)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "DyGEUp8HnbQP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create sentence-level alignments to make the translations\n",
        "# useful to the model\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "GcA4RtcgnbQQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Model Design\n",
        "\n",
        "Since there are a lot of spare documents given alongside the challenge, I would like to try making the model more accurate and consistent through Retrieval Augment Generation(RAG). This way, when an example is being analyzed, the model can take ideas from the transliterated texts provided to better translate what is given. This transformer model will also learn based upon the geometric mean of BLEU and chrF++ scores:\n",
        "\n",
        "$score = \\sqrt{BLEU + chrF}$\n",
        "\n",
        "Since we aim to use a smaller transformer architecture, the ____ model would work best for this problem.\n",
        "\n",
        "We then will use the ____ model for post-processing to make the English translation much better."
      ],
      "metadata": {
        "id": "kMcsGv9YnbQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the models and the RAG retrieval\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "cd1HW-0hnbQQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect the pieces together\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "XS1y4lSgnbQQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Testing the model on the test data\n",
        "\n",
        "Now that we have training data, relevant documents, and a working pipeline, we can now test the model using ```../test.csv```. When the submission occurs, the model will be scored using a hidden set."
      ],
      "metadata": {
        "id": "aKC0a0vEnbQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model on the test set and provide a submissions.csv document\n",
        "# for challenge submission\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "9bj_57n9nbQQ"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}